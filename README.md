# ðŸ•µ Hallucination Detection with Unlabeled Data (Semi-Supervised)

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg) ![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97-HuggingFace-yellow) ![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red.svg)

### The Problem: Silent Failures in LLMs
Large Language Models (LLMs) often generate "fluent but false" information (Hallucinations). In enterprise RAG systems, these errors are dangerous.
**The Challenge:** Detecting these errors usually requires massive labeled datasets, which are expensive and slow to create.

### The Solution
We built a **Semi-Supervised Pipeline** that achieves high detection accuracy **without** needing a fully labeled training set. We use a "Teacher-Student" architecture where a large model creates training data for a smaller, efficient classifier.

---

##  Architecture & Methodology

### 1. The "Teacher": Zero-Shot Pseudo-Labeling
We used **Mistral-7B** (via Zero-Shot Prompting) to act as a "Teacher."
*   We fed it unlabelled text and asked it to identify hallucinations.
*   While Mistral is not perfect, it provided enough "Pseudo-Labels" to bootstrap the training process.

### 2. The "Student": Model Fine-Tuning
We used the synthetic data generated by Mistral to fine-tune a smaller, faster model: **DeBERTa-v3**.
*   **Why DeBERTa?** It is significantly lighter than a GPT model, making it suitable for real-time production monitoring of hallucinations.
*   **Ensemble Strategy:** We combined the DeBERTa output with **CatBoost** (Gradient Boosting) to capture both semantic and statistical features of the text.

---

## Key Results
*   **Data Efficiency:** Successfully trained a robust detector using largely unlabeled data.
*   **Performance:** The ensemble model achieved **73% Accuracy** in distinguishing between fluent text and hallucinations, outperforming the baseline by **14%**.
*   **Real-World Application:** This pipeline proves that companies can build "Safety Monitors" for their AI products without spending months manually labeling data.

---

## Tech Stack
*   **Models:** `Mistral-7B` (Teacher), `DeBERTa-v3` (Student), `CatBoost` (Ensemble)
*   **Libraries:** `PyTorch`, `Transformers`, `Scikit-Learn`
*   **Technique:** `Semi-Supervised Learning`, `Pseudo-Labeling`, `Zero-Shot Prompting`

---
